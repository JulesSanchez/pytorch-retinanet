{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from retinanet import model\n",
    "from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, \\\n",
    "    Normalizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from retinanet import coco_eval\n",
    "from retinanet import csv_eval\n",
    "\n",
    "assert torch.__version__.split('.')[0] == '1'\n",
    "\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (fpn): PyramidFeatures(\n",
       "    (P5_1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (P5_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (P5_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (P4_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (P4_upsampled): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (P4_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (P3_1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (P3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (P6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (P7_1): ReLU()\n",
       "    (P7_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (regressionModel): RegressionModel(\n",
       "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act2): ReLU()\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act3): ReLU()\n",
       "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act4): ReLU()\n",
       "    (output): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (classificationModel): ClassificationModel(\n",
       "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act1): ReLU()\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act2): ReLU()\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act3): ReLU()\n",
       "    (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (act4): ReLU()\n",
       "    (output): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (output_act): Sigmoid()\n",
       "  )\n",
       "  (anchors): Anchors()\n",
       "  (regressBoxes): BBoxTransform()\n",
       "  (clipBoxes): ClipBoxes()\n",
       "  (styleClassificationModel): StyleClassificationModel(\n",
       "    (regressBoxes): BBoxTransform()\n",
       "    (clipBoxes): ClipBoxes()\n",
       "    (linear): Linear(in_features=14, out_features=11, bias=True)\n",
       "    (out): Linear(in_features=11, out_features=4, bias=True)\n",
       "    (non_linear): ReLU()\n",
       "    (non_linear_out): Softmax(dim=None)\n",
       "  )\n",
       "  (focalLoss): FocalLoss()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_inference = 'data/test_retinanet.csv'\n",
    "csv_train = 'data/train_retinanet.csv'\n",
    "csv_classes = 'data/class_retinanet.csv'\n",
    "model_path = 'csv_retinanet_68.pt'\n",
    "dataset_inference = CSVDataset(train_file=csv_inference, class_list=csv_classes,\n",
    "                                   transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "sampler = AspectRatioBasedSampler(dataset_inference, batch_size=1, drop_last=False)\n",
    "dataloader_inference = DataLoader(dataset_inference, num_workers=3, collate_fn=collater, batch_sampler=sampler)\n",
    "\n",
    "\n",
    "dataset_train = CSVDataset(train_file=csv_train, class_list=csv_classes,\n",
    "                                   transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "sampler = AspectRatioBasedSampler(dataset_train, batch_size=1, drop_last=False)\n",
    "dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater, batch_sampler=sampler)\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "retinanet = torch.load(model_path)\n",
    "retinanet = retinanet.cuda()\n",
    "retinanet.training = False\n",
    "retinanet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "classifier = retinanet.styleClassificationModel\n",
    "classifier.eval()\n",
    "retinanet.style_inference = True\n",
    "from scipy.special import softmax\n",
    "def predict(feature_vec):\n",
    "        res = []\n",
    "        for el in feature_vec:\n",
    "            partial_res = classifier.out(classifier.non_linear(classifier.linear(torch.tensor(el).cuda()))).detach().cpu().numpy()\n",
    "            res.append(softmax(partial_res))\n",
    "        return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14)\n",
      "(14,)\n",
      "(4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(np.array([test_data[0]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 196 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "  0%|          | 1/219 [00:00<00:25,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 4.575468    0.83224285  1.2297149  ...  0.3909327   0.72463787\n",
      "    0.04059374]]\n",
      "\n",
      " [[ 9.304609    0.24425282  0.5528246  ...  0.0826336   0.21650867\n",
      "    0.11110298]]\n",
      "\n",
      " [[ 3.9160197   0.19689088  0.3973506  ...  0.06380919  0.15808202\n",
      "    0.03418554]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.411285    0.54745793  3.17768    ...  1.273254    4.4564133\n",
      "    3.2533772 ]]\n",
      "\n",
      " [[10.249314    0.41098285  1.1365355  ...  0.14434719  0.95457923\n",
      "    0.01472451]]\n",
      "\n",
      " [[ 0.23575357  0.0997358   0.27453765 ...  0.36648452  0.9224097\n",
      "    0.06749362]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:20<00:00, 10.82it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "for index in range(len(dataset_train)):\n",
    "    data = dataset_train[index]\n",
    "    [_,_,_], style, feature_vec = retinanet(data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0))\n",
    "    train_data.append(feature_vec.detach().cpu().numpy())\n",
    "styles = []\n",
    "styles_bis = []\n",
    "for index in range(len(dataset_inference)):\n",
    "    data = dataset_inference[index]\n",
    "    [_,_,_], style, feature_vec = retinanet(data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0))\n",
    "    styles.append(np.squeeze(style.detach().cpu().numpy()))\n",
    "    test_data.append(feature_vec.detach().cpu().numpy())\n",
    "    pred = predict(test_data[index])\n",
    "    styles_bis.append(np.squeeze(pred))\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.squeeze(train_data)\n",
    "test_data = np.squeeze(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 196 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "  6%|▋         | 14/219 [00:17<04:30,  1.32s/it]Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.725e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.842e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.982e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.318e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.426e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.626e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.684e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.990e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.209e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.588e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.024e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.314e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.879e-07, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  8%|▊         | 18/219 [00:22<04:20,  1.30s/it]Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.673e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.427e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=1.829e-04, previous alpha=8.313e-05, with an active set of 12 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=6.405e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.207e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.203e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      " 18%|█▊        | 39/219 [00:53<04:21,  1.46s/it]Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.785e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.748e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.053e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.074e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.936e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.399e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.822e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.694e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.298e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.524e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.792e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.829e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.509e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.489e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.887e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.243e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 55/219 [01:15<03:54,  1.43s/it]Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.435e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.726e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.797e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.378e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.656e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      " 29%|██▉       | 64/219 [01:28<03:44,  1.45s/it]Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=9.646e-04, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.245e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.053e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.879e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      " 30%|███       | 66/219 [01:31<03:40,  1.44s/it]Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.256e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.595e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=9.607e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=3.406e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=1.505e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.483e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.074e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=6.711e-05, previous alpha=2.837e-05, with an active set of 12 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.155e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.089e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.535e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=3.189e-04, previous alpha=1.077e-04, with an active set of 11 regressors.\n",
      " 34%|███▍      | 74/219 [01:43<03:31,  1.46s/it]Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.697e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.640e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.565e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.057e-06, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.672e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.988e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.402e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      " 44%|████▍     | 97/219 [02:16<02:59,  1.47s/it]Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=3.571e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.277e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=8.412e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      " 47%|████▋     | 103/219 [02:25<02:52,  1.49s/it]Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.689e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.185e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=6.889e-06, previous alpha=6.799e-06, with an active set of 12 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.286e-05, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=9.788e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.930e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.571e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.776e-07, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.801e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 109/219 [02:34<02:39,  1.45s/it]Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.165e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.625e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.560e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.307e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.360e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.037e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.918e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.219e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.139e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.721e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.768e-06, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      " 57%|█████▋    | 125/219 [02:58<02:17,  1.46s/it]Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.417e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.273e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.617e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.025e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.320e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.919e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.232e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.955e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      " 58%|█████▊    | 126/219 [02:59<02:15,  1.45s/it]Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.837e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.363e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.823e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.509e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.746e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.130e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=4.039e-04, previous alpha=6.510e-05, with an active set of 12 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.548e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.443e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.718e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=6.873e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.416e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=4.390e-05, previous alpha=3.532e-05, with an active set of 10 regressors.\n",
      " 61%|██████    | 133/219 [03:09<02:05,  1.46s/it]Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.846e-04, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.146e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=9.809e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.382e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.368e-07, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.220e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=7.748e-06, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 134/219 [03:11<02:06,  1.48s/it]Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.380e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.399e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.532e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.201e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.300e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=5.348e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.268e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.045e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=1.995e-04, previous alpha=8.646e-05, with an active set of 11 regressors.\n",
      " 67%|██████▋   | 147/219 [03:30<01:50,  1.53s/it]Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.292e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.461e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.168e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.192e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=1.183e-03, previous alpha=8.274e-04, with an active set of 9 regressors.\n",
      " 71%|███████   | 156/219 [03:45<01:41,  1.62s/it]Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.602e-03, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.248e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.276e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.682e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.586e-06, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.709e-04, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.342e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.375e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=2.369e-04, previous alpha=1.730e-04, with an active set of 8 regressors.\n",
      " 74%|███████▎  | 161/219 [03:52<01:27,  1.52s/it]Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.556e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.101e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.250e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.485e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.221e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      " 81%|████████  | 177/219 [04:17<01:06,  1.58s/it]Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.999e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.000e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.188e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.465e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.689e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.108e-05, with an active set of 9 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.803e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.164e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.470e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.368e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.871e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.755e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.628e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.526e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.206e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.642e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=2.112e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 190/219 [04:37<00:44,  1.54s/it]Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.656e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=8.051e-05, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.221e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=2.796e-05, previous alpha=1.961e-05, with an active set of 11 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.542e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.411e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.105e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.882e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.154e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=6.663e-06, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.087e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.090e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      " 89%|████████▉ | 196/219 [04:45<00:32,  1.42s/it]Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.313e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.719e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.389e-07, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.295e-06, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=1.273e-06, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.358e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.493e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.106e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.485e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.325e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.114e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.343e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      " 98%|█████████▊| 214/219 [05:11<00:07,  1.47s/it]Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=5.103e-03, with an active set of 1 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.248e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.361e-06, with an active set of 12 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.192e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=6.084e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.147e-04, with an active set of 6 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.957e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=1.743e-04, previous alpha=7.867e-05, with an active set of 12 regressors.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.034e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.307e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [05:19<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "elements = np.random.choice(len(train_data), int(0.3*len(train_data)), False)\n",
    "explainer = shap.KernelExplainer(predict, train_data[elements])\n",
    "shap_values_test = explainer.shap_values(test_data, nsamples=30, l1_reg='bic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.575468  , 0.83224285, 1.2297149 , 1.7188413 , 1.2518927 ,\n",
       "       1.1866384 , 0.40216124, 0.6808334 , 0.21566421, 0.24898162,\n",
       "       0.26711395, 0.3909327 , 0.72463787, 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(219, 14)\n"
     ]
    }
   ],
   "source": [
    "print(shap_values_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../architectural_style_classification\")\n",
    "from utils.knowledge_graph import GED_metric\n",
    "d = GED_metric(test_data, shap_values_test, dataset='MonumenAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.634703196347032"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondab003be3b3cc14792a70909b28513a836"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
